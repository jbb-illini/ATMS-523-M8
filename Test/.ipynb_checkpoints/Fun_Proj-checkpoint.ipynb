{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cda532-a7ce-47aa-9d83-1323b19c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Solar Power Forecasting with Random Forest and Gradient Boosting\n",
    "Author: Joey Bahret\n",
    "Date: November 11, 2025\n",
    "\n",
    "This script forecasts daily solar power production using historical PV data\n",
    "and meteorological features from NSRDB. Improvements include better feature\n",
    "engineering, multiple models, comprehensive evaluation, and proper code structure.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import io\n",
    "import requests\n",
    "import urllib.parse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data sources\n",
    "    's3_base_uri': 's3://oedi-data-lake/pvdaq/csv/pvdata/system_id=11487/',\n",
    "    'nrel_api_key': 'kqrttpYYUcxix5pCW7wO5pysSE8wJ0XaYShbxnDf',\n",
    "    'nrel_email': 'jbahret2@illinois.edu',\n",
    "    'nrel_base_url': 'https://developer.nrel.gov/api/nsrdb/v2/solar/nsrdb-GOES-aggregated-v4-0-0-download.json?',\n",
    "    'location_id': '205581',\n",
    "    'data_directory': '/Users/joeybahret/Documents/Grad_School/ATMS_523/Module 8/nsrdb_data/',\n",
    "    \n",
    "    # Model parameters\n",
    "    'target_column': 'Power_Mean',\n",
    "    'train_split': 0.8,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Feature engineering\n",
    "    'rolling_windows': [3, 7, 14],\n",
    "    'lag_days': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_pv_data_from_s3(s3_uri, verbose=True):\n",
    "    \"\"\"\n",
    "    Load PV data from S3 bucket and combine all CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    s3_uri : str\n",
    "        Base S3 URI for the PV data\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined PV data with datetime index\n",
    "    \"\"\"\n",
    "    s3_wildcard_path = s3_uri + '**/*.csv'\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    \n",
    "    try:\n",
    "        csv_paths = fs.glob(s3_wildcard_path)\n",
    "        csv_files = [f's3://{path}' for path in csv_paths]\n",
    "        \n",
    "        if not csv_files:\n",
    "            raise ValueError(\"No CSV files found using the wildcard path.\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Found {len(csv_files)} files. Loading data...\")\n",
    "        \n",
    "        # Identify date column from first file\n",
    "        with fs.open(csv_files[0], 'rb') as f:\n",
    "            temp_df = pd.read_csv(io.BytesIO(f.read()), nrows=5)\n",
    "            all_cols = temp_df.columns.tolist()\n",
    "        \n",
    "        time_candidates = ['time', 'date', 'datetime', 'dt_utc', 'dt_iso', 'ts', 'Unnamed: 0']\n",
    "        date_col = next((col for col in all_cols if col.lower() in time_candidates), all_cols[0])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Identified timestamp column: '{date_col}'\")\n",
    "        \n",
    "        # Read and concatenate all files\n",
    "        df_list = []\n",
    "        for file in csv_files:\n",
    "            df_list.append(pd.read_csv(file, parse_dates=[date_col]))\n",
    "        \n",
    "        pv_data = pd.concat(df_list, ignore_index=True)\n",
    "        pv_data = pv_data.sort_values(by=date_col).reset_index(drop=True)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        pv_data.rename(columns={\n",
    "            date_col: 'Date',\n",
    "            'ac_power_inv_21445_daily_max': 'Power_Max',\n",
    "            'ac_power_inv_21445_daily_mean': 'Power_Mean',\n",
    "            'ac_energy_inv_21445_daily_sum': 'Total_kWh'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Set date index\n",
    "        pv_data['Date'] = pd.to_datetime(pv_data['Date'])\n",
    "        pv_data = pv_data.set_index('Date').sort_index()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Successfully loaded {len(pv_data)} rows of PV data\")\n",
    "            print(f\"Date range: {pv_data.index.min()} to {pv_data.index.max()}\")\n",
    "        \n",
    "        return pv_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PV data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def request_nsrdb_data(api_key, email, base_url, location_id, years, verbose=True):\n",
    "    \"\"\"\n",
    "    Request NSRDB data from NREL API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_key : str\n",
    "        NREL API key\n",
    "    email : str\n",
    "        User email\n",
    "    base_url : str\n",
    "        API endpoint URL\n",
    "    location_id : str\n",
    "        Location ID for data request\n",
    "    years : list\n",
    "        List of years to request\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of download URLs\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'attributes': 'air_temperature,cloud_type,clearsky_ghi,ghi,ghuv-295-385,wind_speed',\n",
    "        'interval': '60',\n",
    "        'include_leap_day': 'true',\n",
    "        'api_key': api_key,\n",
    "        'email': email,\n",
    "        'location_ids': location_id\n",
    "    }\n",
    "    \n",
    "    download_urls = []\n",
    "    \n",
    "    for year in years:\n",
    "        if verbose:\n",
    "            print(f\"Requesting data for year: {year}\")\n",
    "        \n",
    "        input_data['names'] = [str(year)]\n",
    "        \n",
    "        try:\n",
    "            headers = {'x-api-key': api_key}\n",
    "            response = requests.post(base_url, input_data, headers=headers, timeout=30)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error: Status code {response.status_code} for year {year}\")\n",
    "                continue\n",
    "            \n",
    "            response_json = response.json()\n",
    "            \n",
    "            if len(response_json.get('errors', [])) > 0:\n",
    "                print(f\"API Errors for {year}: {response_json['errors']}\")\n",
    "                continue\n",
    "            \n",
    "            download_url = response_json['outputs']['downloadUrl']\n",
    "            download_urls.append((year, download_url))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  ✓ Request successful: {response_json['outputs']['message']}\")\n",
    "            \n",
    "            # Exponential backoff for rate limiting\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to request data for year {year}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return download_urls\n",
    "\n",
    "\n",
    "def load_nsrdb_data(data_directory, years, location_info='205581_34.61_-118.18', verbose=True):\n",
    "    \"\"\"\n",
    "    Load NSRDB data from local CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_directory : str\n",
    "        Directory containing NSRDB CSV files\n",
    "    years : list\n",
    "        List of years to load\n",
    "    location_info : str\n",
    "        Location identifier in filename\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined weather data with datetime index\n",
    "    \"\"\"\n",
    "    files_to_read = []\n",
    "    for year in years:\n",
    "        filename = f'{location_info}_{year}.csv'\n",
    "        files_to_read.append(data_directory + filename)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loading {len(files_to_read)} NSRDB files...\")\n",
    "    \n",
    "    # Read and combine all files (skip first 2 rows with metadata)\n",
    "    combined_df = pd.concat(\n",
    "        (pd.read_csv(f, header=2) for f in files_to_read),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Create datetime index\n",
    "    date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    combined_df['Timestamp'] = pd.to_datetime(combined_df[date_cols])\n",
    "    combined_df = combined_df.set_index('Timestamp').drop(columns=date_cols)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(combined_df)} hourly weather records\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def aggregate_to_daily(hourly_df, verbose=True):\n",
    "    \"\"\"\n",
    "    Aggregate hourly weather data to daily means.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hourly_df : pd.DataFrame\n",
    "        Hourly weather data\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Daily aggregated weather data\n",
    "    \"\"\"\n",
    "    daily_df = hourly_df.resample('D').mean()\n",
    "    daily_df.index.rename('Date', inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Aggregated to {len(daily_df)} daily records\")\n",
    "    \n",
    "    return daily_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def clean_pv_data(pv_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Clean PV data by handling missing values and outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pv_data : pd.DataFrame\n",
    "        Raw PV data\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned PV data\n",
    "    \"\"\"\n",
    "    df = pv_data.copy()\n",
    "    \n",
    "    # Check for missing data\n",
    "    missing_pct = (df['Total_kWh'].isna().sum() / len(df)) * 100\n",
    "    if verbose and missing_pct > 0:\n",
    "        print(f\"Missing data: {missing_pct:.2f}% of Total_kWh values\")\n",
    "    \n",
    "    # For short gaps (1-2 days), use interpolation instead of filling with 0\n",
    "    df['Total_kWh'] = df['Total_kWh'].interpolate(method='time', limit=2)\n",
    "    \n",
    "    # Fill remaining NaNs with 0 (likely longer outages)\n",
    "    df['Total_kWh'] = df['Total_kWh'].fillna(0)\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    df.loc[df['Total_kWh'] < 0, 'Total_kWh'] = 0\n",
    "    df.loc[df['Power_Mean'] < 0, 'Power_Mean'] = 0\n",
    "    df.loc[df['Power_Max'] < 0, 'Power_Max'] = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"PV data cleaned and validated\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create temporal features from datetime index.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with datetime index\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added temporal features\n",
    "    \"\"\"\n",
    "    df['Day_of_Year'] = df.index.dayofyear\n",
    "    df['Day_of_Week'] = df.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['Month'] = df.index.month\n",
    "    df['Week_of_Year'] = df.index.isocalendar().week\n",
    "    \n",
    "    # Cyclical encoding for day of year\n",
    "    df['sin_day'] = np.sin(2 * np.pi * df.index.dayofyear / 365.25)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df.index.dayofyear / 365.25)\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['Day_of_Week'] >= 5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, columns, lags=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Create lagged features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    columns : list\n",
    "        List of column names to create lags for\n",
    "    lags : list\n",
    "        List of lag periods (in days)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added lagged features\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}D'] = df[col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rolling_features(df, columns, windows=[3, 7, 14]):\n",
    "    \"\"\"\n",
    "    Create rolling window features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    columns : list\n",
    "        List of column names to create rolling features for\n",
    "    windows : list\n",
    "        List of window sizes (in days)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added rolling features\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        for window in windows:\n",
    "            df[f'{col}_rolling_{window}d'] = df[col].rolling(window).mean()\n",
    "            df[f'{col}_std_{window}d'] = df[col].rolling(window).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_difference_features(df, columns):\n",
    "    \"\"\"\n",
    "    Create day-to-day difference features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    columns : list\n",
    "        List of column names to create differences for\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added difference features\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[f'{col}_diff'] = df[col].diff()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_all_features(pv_data, weather_data, config, verbose=True):\n",
    "    \"\"\"\n",
    "    Combine PV and weather data and engineer all features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pv_data : pd.DataFrame\n",
    "        PV production data\n",
    "    weather_data : pd.DataFrame\n",
    "        Weather data\n",
    "    config : dict\n",
    "        Configuration dictionary\n",
    "    verbose : bool\n",
    "        Print progress messages\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataset with engineered features\n",
    "    \"\"\"\n",
    "    # Join datasets\n",
    "    df = pv_data.join(weather_data, how='inner')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Combined dataset: {len(df)} rows from {df.index.min()} to {df.index.max()}\")\n",
    "    \n",
    "    # Create temporal features\n",
    "    df = create_temporal_features(df)\n",
    "    \n",
    "    # Create lagged features\n",
    "    lag_columns = [config['target_column'], 'GHI', 'Temperature', 'Wind Speed']\n",
    "    df = create_lagged_features(df, lag_columns, config['lag_days'])\n",
    "    \n",
    "    # Create rolling features\n",
    "    rolling_columns = [config['target_column'], 'GHI', 'Temperature']\n",
    "    df = create_rolling_features(df, rolling_columns, config['rolling_windows'])\n",
    "    \n",
    "    # Create difference features (change from previous day)\n",
    "    diff_columns = ['GHI', 'Temperature', 'Wind Speed']\n",
    "    df = create_difference_features(df, diff_columns)\n",
    "    \n",
    "    # Drop rows with NaN values created by lagging/rolling\n",
    "    initial_len = len(df)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"After feature engineering: {len(df)} rows ({initial_len - len(df)} dropped due to NaN)\")\n",
    "        print(f\"Total features: {len(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING & EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_features_target(df, target_col, feature_type='forecast'):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix (X) and target vector (y).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Full dataset with all features\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    feature_type : str\n",
    "        'forecast' for true forecast (only lagged features) or \n",
    "        'nowcast' for same-day features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target vector\n",
    "    \"\"\"\n",
    "    y = df[target_col]\n",
    "    \n",
    "    if feature_type == 'forecast':\n",
    "        # Use only features known ahead of time (lagged features + temporal)\n",
    "        feature_cols = [col for col in df.columns if \n",
    "                       ('lag' in col or 'rolling' in col or 'std' in col or \n",
    "                        'sin_' in col or 'cos_' in col or 'diff' in col or\n",
    "                        col in ['Day_of_Year', 'Day_of_Week', 'Month', 'Week_of_Year', 'is_weekend'])\n",
    "                       and col != target_col]\n",
    "    else:\n",
    "        # Use all features including same-day weather (for comparison)\n",
    "        exclude_cols = [target_col, 'Total_kWh', 'Power_Max']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def time_series_split(X, y, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split time series data into train and test sets chronologically.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target vector\n",
    "    train_ratio : float\n",
    "        Proportion of data for training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test : tuple\n",
    "        Split datasets\n",
    "    \"\"\"\n",
    "    split_point = int(len(X) * train_ratio)\n",
    "    \n",
    "    X_train = X[:split_point]\n",
    "    X_test = X[split_point:]\n",
    "    y_train = y[:split_point]\n",
    "    y_test = y[split_point:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def train_random_forest(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Train Random Forest model with optimized hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training target\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : RandomForestRegressor\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Train Gradient Boosting model with optimized hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training target\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : GradientBoostingRegressor\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        subsample=0.8,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'Correlation': np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    }\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() > 0:\n",
    "        metrics['MAPE'] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    else:\n",
    "        metrics['MAPE'] = np.nan\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Evaluate model and print metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test target\n",
    "    model_name : str\n",
    "        Name of model for display\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : array\n",
    "        Predictions\n",
    "    metrics : dict\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Performance on Test Set\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"RMSE:        {metrics['RMSE']:.4f}\")\n",
    "    print(f\"MAE:         {metrics['MAE']:.4f}\")\n",
    "    print(f\"R²:          {metrics['R2']:.4f}\")\n",
    "    print(f\"Correlation: {metrics['Correlation']:.4f}\")\n",
    "    print(f\"MAPE:        {metrics['MAPE']:.2f}%\")\n",
    "    \n",
    "    return y_pred, metrics\n",
    "\n",
    "\n",
    "def evaluate_naive_baseline(X_test, y_test, lag_col='Power_Mean_lag_1D'):\n",
    "    \"\"\"\n",
    "    Evaluate naive persistence forecast as baseline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test target\n",
    "    lag_col : str\n",
    "        Column name for lag-1 prediction\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Evaluation metrics for naive model\n",
    "    \"\"\"\n",
    "    y_pred_naive = X_test[lag_col]\n",
    "    metrics = calculate_metrics(y_test, y_pred_naive)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Naive Persistence Baseline Performance\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"RMSE:        {metrics['RMSE']:.4f}\")\n",
    "    print(f\"MAE:         {metrics['MAE']:.4f}\")\n",
    "    print(f\"R²:          {metrics['R2']:.4f}\")\n",
    "    print(f\"Correlation: {metrics['Correlation']:.4f}\")\n",
    "    print(f\"MAPE:        {metrics['MAPE']:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_skill_score(model_rmse, baseline_rmse):\n",
    "    \"\"\"\n",
    "    Calculate forecast skill score relative to baseline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_rmse : float\n",
    "        RMSE of the model\n",
    "    baseline_rmse : float\n",
    "        RMSE of the baseline\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Skill score (0-1, higher is better)\n",
    "    \"\"\"\n",
    "    return 1 - (model_rmse / baseline_rmse)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_predictions(y_test, y_pred, y_pred_naive=None, model_name='Model', \n",
    "                     r2_score_val=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values over time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : pd.Series\n",
    "        Actual test values\n",
    "    y_pred : array\n",
    "        Model predictions\n",
    "    y_pred_naive : array, optional\n",
    "        Naive baseline predictions\n",
    "    model_name : str\n",
    "        Name of model for title\n",
    "    r2_score_val : float, optional\n",
    "        R² score for legend\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    plt.plot(y_test.index, y_test, label='Actual Power', \n",
    "             color='black', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    label = f'{model_name} Forecast'\n",
    "    if r2_score_val is not None:\n",
    "        label += f' (R²={r2_score_val:.3f})'\n",
    "    plt.plot(y_test.index, y_pred, label=label,\n",
    "             color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    if y_pred_naive is not None:\n",
    "        plt.plot(y_test.index, y_pred_naive, \n",
    "                label='Naive Forecast (Persistence)',\n",
    "                color='gray', linestyle=':', linewidth=1.2, alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Daily Power Forecast: Actual vs. {model_name} (Test Set)', fontsize=14)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Daily Mean Power (kW)', fontsize=12)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, linestyle=':', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_residuals(y_test, y_pred, model_name='Model', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot residuals over time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : pd.Series\n",
    "        Actual test values\n",
    "    y_pred : array\n",
    "        Model predictions\n",
    "    model_name : str\n",
    "        Name of model for title\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Residuals over time\n",
    "    axes[0].scatter(y_test.index, residuals, alpha=0.5, s=20)\n",
    "    axes[0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0].set_title(f'{model_name} Residuals Over Time', fontsize=13)\n",
    "    axes[0].set_xlabel('Date', fontsize=11)\n",
    "    axes[0].set_ylabel('Error (Actual - Predicted)', fontsize=11)\n",
    "    axes[0].grid(True, linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Residual histogram\n",
    "    axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].set_title('Residual Distribution', fontsize=13)\n",
    "    axes[1].set_xlabel('Residual', fontsize=11)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1].grid(True, linestyle=':', alpha=0.5, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=15, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance from tree-based model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model with feature_importances_\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    top_n : int\n",
    "        Number of top features to display\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(top_n), importances[indices], color='steelblue')\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Feature Importances', fontsize=14)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', linestyle=':', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_summary(model, X_test, save_path=None):\n",
    "    \"\"\"\n",
    "    Create SHAP summary plot.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    # Sample data if too large (SHAP can be slow)\n",
    "    if len(X_test) > 500:\n",
    "        X_test_sample = X_test.sample(500, random_state=42)\n",
    "    else:\n",
    "        X_test_sample = X_test\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def plot_shap_importance_bar(shap_values, X_test, top_n=10, save_path=None):\n",
    "    \"\"\"\n",
    "    Create bar plot of mean absolute SHAP values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shap_values : array\n",
    "        SHAP values from explainer\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    top_n : int\n",
    "        Number of top features to display\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Mean_SHAP': mean_abs_shap\n",
    "    }).sort_values(by='Mean_SHAP', ascending=True).tail(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Mean_SHAP'], \n",
    "             color='steelblue')\n",
    "    plt.xlabel('Mean |SHAP value|', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Features by SHAP Importance', fontsize=14)\n",
    "    plt.grid(axis='x', linestyle=':', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_seasonal_performance(y_test, y_pred, save_path=None):\n",
    "    \"\"\"\n",
    "    Analyze model performance by season.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : pd.Series\n",
    "        Actual test values\n",
    "    y_pred : array\n",
    "        Model predictions\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Error': y_test - y_pred,\n",
    "        'AbsError': np.abs(y_test - y_pred),\n",
    "        'Month': y_test.index.month\n",
    "    })\n",
    "    \n",
    "    # Define seasons\n",
    "    season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                  3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                  6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                  9: 'Fall', 10: 'Fall', 11: 'Fall'}\n",
    "    df['Season'] = df['Month'].map(season_map)\n",
    "    \n",
    "    # Calculate metrics by season\n",
    "    seasonal_metrics = df.groupby('Season').agg({\n",
    "        'AbsError': 'mean',\n",
    "        'Error': 'std'\n",
    "    }).reindex(['Spring', 'Summer', 'Fall', 'Winter'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # MAE by season\n",
    "    axes[0].bar(seasonal_metrics.index, seasonal_metrics['AbsError'], \n",
    "                color=['#90EE90', '#FFD700', '#FF8C00', '#87CEEB'])\n",
    "    axes[0].set_ylabel('Mean Absolute Error', fontsize=11)\n",
    "    axes[0].set_title('Forecast Error by Season', fontsize=13)\n",
    "    axes[0].grid(axis='y', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Error std by season\n",
    "    axes[1].bar(seasonal_metrics.index, seasonal_metrics['Error'], \n",
    "                color=['#90EE90', '#FFD700', '#FF8C00', '#87CEEB'])\n",
    "    axes[1].set_ylabel('Error Standard Deviation', fontsize=11)\n",
    "    axes[1].set_title('Error Variability by Season', fontsize=13)\n",
    "    axes[1].grid(axis='y', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSeasonal Performance Metrics:\")\n",
    "    print(seasonal_metrics)\n",
    "\n",
    "\n",
    "def compare_models(results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary with model names as keys and metrics dicts as values\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    metrics_df = pd.DataFrame(results_dict).T\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    metrics_to_plot = ['RMSE', 'MAE', 'R2', 'MAPE']\n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "    \n",
    "    for idx, (metric, ax, color) in enumerate(zip(metrics_to_plot, axes.flat, colors)):\n",
    "        if metric in metrics_df.columns:\n",
    "            values = metrics_df[metric]\n",
    "            bars = ax.barh(values.index, values, color=color, alpha=0.7)\n",
    "            ax.set_xlabel(metric, fontsize=11)\n",
    "            ax.set_title(f'Model Comparison: {metric}', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', linestyle=':', alpha=0.5)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                width = bar.get_width()\n",
    "                ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{width:.4f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(metrics_df.to_string())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for solar power forecasting.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"SOLAR POWER FORECASTING - IMPROVED VERSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. LOAD DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[1/7] Loading PV data from S3...\")\n",
    "    pv_data = load_pv_data_from_s3(CONFIG['s3_base_uri'])\n",
    "    \n",
    "    print(\"\\n[2/7] Loading NSRDB weather data...\")\n",
    "    years = list(range(2016, 2025))\n",
    "    weather_hourly = load_nsrdb_data(\n",
    "        CONFIG['data_directory'],\n",
    "        years,\n",
    "        location_info='205581_34.61_-118.18'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[3/7] Aggregating weather data to daily...\")\n",
    "    weather_daily = aggregate_to_daily(weather_hourly)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. PREPROCESS & FEATURE ENGINEERING\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[4/7] Cleaning and engineering features...\")\n",
    "    pv_data_clean = clean_pv_data(pv_data)\n",
    "    \n",
    "    full_data = engineer_all_features(\n",
    "        pv_data_clean,\n",
    "        weather_daily,\n",
    "        CONFIG\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {full_data.shape}\")\n",
    "    print(f\"Features created: {len(full_data.columns)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. PREPARE FEATURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[5/7] Preparing features and target...\")\n",
    "    \n",
    "    # Forecast features (only lagged/known ahead)\n",
    "    X_forecast, y = prepare_features_target(\n",
    "        full_data,\n",
    "        CONFIG['target_column'],\n",
    "        feature_type='forecast'\n",
    "    )\n",
    "    \n",
    "    print(f\"Forecast features: {len(X_forecast.columns)}\")\n",
    "    print(f\"Sample features: {list(X_forecast.columns[:10])}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = time_series_split(\n",
    "        X_forecast, y,\n",
    "        train_ratio=CONFIG['train_split']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    print(f\"Test period: {X_test.index[0]} to {X_test.index[-1]}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. TRAIN MODELS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[6/7] Training models...\")\n",
    "    \n",
    "    print(\"\\n  Training Random Forest...\")\n",
    "    rf_model = train_random_forest(X_train, y_train, CONFIG['random_state'])\n",
    "    \n",
    "    print(\"  Training Gradient Boosting...\")\n",
    "    gb_model = train_gradient_boosting(X_train, y_train, CONFIG['random_state'])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. EVALUATE MODELS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n[7/7] Evaluating models...\")\n",
    "    \n",
    "    # Evaluate Random Forest\n",
    "    y_pred_rf, metrics_rf = evaluate_model(\n",
    "        rf_model, X_test, y_test,\n",
    "        model_name='Random Forest'\n",
    "    )\n",
    "    \n",
    "    # Evaluate Gradient Boosting\n",
    "    y_pred_gb, metrics_gb = evaluate_model(\n",
    "        gb_model, X_test, y_test,\n",
    "        model_name='Gradient Boosting'\n",
    "    )\n",
    "    \n",
    "    # Evaluate Naive Baseline\n",
    "    metrics_naive = evaluate_naive_baseline(X_test, y_test)\n",
    "    \n",
    "    # Calculate skill scores\n",
    "    skill_rf = calculate_skill_score(metrics_rf['RMSE'], metrics_naive['RMSE'])\n",
    "    skill_gb = calculate_skill_score(metrics_gb['RMSE'], metrics_naive['RMSE'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SKILL SCORES (vs. Naive Baseline)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Random Forest:      {skill_rf:.4f} ({skill_rf*100:.2f}% improvement)\")\n",
    "    print(f\"Gradient Boosting:  {skill_gb:.4f} ({skill_gb*100:.2f}% improvement)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6. VISUALIZATIONS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Compare all models\n",
    "    results = {\n",
    "        'Naive Baseline': metrics_naive,\n",
    "        'Random Forest': metrics_rf,\n",
    "        'Gradient Boosting': metrics_gb\n",
    "    }\n",
    "    compare_models(results, save_path='model_comparison.png')\n",
    "    \n",
    "    # Choose best model for detailed analysis\n",
    "    best_model = gb_model if metrics_gb['R2'] > metrics_rf['R2'] else rf_model\n",
    "    best_pred = y_pred_gb if metrics_gb['R2'] > metrics_rf['R2'] else y_pred_rf\n",
    "    best_name = 'Gradient Boosting' if metrics_gb['R2'] > metrics_rf['R2'] else 'Random Forest'\n",
    "    best_r2 = metrics_gb['R2'] if metrics_gb['R2'] > metrics_rf['R2'] else metrics_rf['R2']\n",
    "    \n",
    "    print(f\"\\nBest model: {best_name} (R² = {best_r2:.4f})\")\n",
    "    \n",
    "    # Predictions plot\n",
    "    print(\"\\nCreating prediction plots...\")\n",
    "    y_pred_naive = X_test['Power_Mean_lag_1D'].values\n",
    "    plot_predictions(\n",
    "        y_test, best_pred, y_pred_naive,\n",
    "        model_name=best_name,\n",
    "        r2_score_val=best_r2,\n",
    "        save_path='predictions_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # Residuals plot\n",
    "    print(\"Creating residual plots...\")\n",
    "    plot_residuals(\n",
    "        y_test, best_pred,\n",
    "        model_name=best_name,\n",
    "        save_path='residuals_analysis.png'\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"Creating feature importance plots...\")\n",
    "    plot_feature_importance(\n",
    "        best_model,\n",
    "        X_test.columns,\n",
    "        top_n=15,\n",
    "        save_path='feature_importance.png'\n",
    "    )\n",
    "    \n",
    "    # SHAP analysis\n",
    "    print(\"Performing SHAP analysis (this may take a moment)...\")\n",
    "    shap_values = plot_shap_summary(\n",
    "        best_model,\n",
    "        X_test,\n",
    "        save_path='shap_summary.png'\n",
    "    )\n",
    "    \n",
    "    plot_shap_importance_bar(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        top_n=10,\n",
    "        save_path='shap_importance_bar.png'\n",
    "    )\n",
    "    \n",
    "    # Seasonal performance\n",
    "    print(\"Analyzing seasonal performance...\")\n",
    "    plot_seasonal_performance(\n",
    "        y_test, best_pred,\n",
    "        save_path='seasonal_performance.png'\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7. ERROR ANALYSIS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOP 5 DAYS WITH LARGEST FORECAST ERRORS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    error_analysis = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': best_pred,\n",
    "        'Error': y_test - best_pred,\n",
    "        'AbsError': np.abs(y_test - best_pred),\n",
    "        'PctError': np.abs((y_test - best_pred) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    worst_days = error_analysis.nlargest(5, 'AbsError')\n",
    "    print(worst_days[['Actual', 'Predicted', 'Error', 'PctError']].to_string())\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8. SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FORECAST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTarget Variable: {CONFIG['target_column']}\")\n",
    "    print(f\"Training Period: {X_train.index[0]} to {X_train.index[-1]}\")\n",
    "    print(f\"Test Period: {X_test.index[0]} to {X_test.index[-1]}\")\n",
    "    print(f\"\\nBest Model: {best_name}\")\n",
    "    print(f\"  - RMSE: {metrics_gb['RMSE'] if best_name == 'Gradient Boosting' else metrics_rf['RMSE']:.4f}\")\n",
    "    print(f\"  - MAE: {metrics_gb['MAE'] if best_name == 'Gradient Boosting' else metrics_rf['MAE']:.4f}\")\n",
    "    print(f\"  - R²: {best_r2:.4f}\")\n",
    "    print(f\"  - MAPE: {metrics_gb['MAPE'] if best_name == 'Gradient Boosting' else metrics_rf['MAPE']:.2f}%\")\n",
    "    print(f\"  - Skill Score: {skill_gb if best_name == 'Gradient Boosting' else skill_rf:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS COMPLETE - All visualizations saved!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        'models': {'rf': rf_model, 'gb': gb_model},\n",
    "        'predictions': {'rf': y_pred_rf, 'gb': y_pred_gb, 'naive': y_pred_naive},\n",
    "        'metrics': results,\n",
    "        'data': {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test},\n",
    "        'best_model': best_model,\n",
    "        'best_name': best_name\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def predict_next_day(model, latest_data, feature_cols):\n",
    "    \"\"\"\n",
    "    Make a prediction for the next day given the latest available data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained forecasting model\n",
    "    latest_data : pd.DataFrame\n",
    "        Most recent data with all required features\n",
    "    feature_cols : list\n",
    "        List of feature column names used in training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Predicted power for next day\n",
    "    \"\"\"\n",
    "    X_next = latest_data[feature_cols].iloc[-1:].values\n",
    "    prediction = model.predict(X_next)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    \"\"\"\n",
    "    Save trained model to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model\n",
    "    filepath : str\n",
    "        Path to save model\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "\n",
    "def load_model(filepath):\n",
    "    \"\"\"\n",
    "    Load trained model from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to saved model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : sklearn model\n",
    "        Loaded model\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    model = joblib.load(filepath)\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_forecast_report(y_test, y_pred, metrics, save_path='forecast_report.txt'):\n",
    "    \"\"\"\n",
    "    Create a text report of forecast results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : pd.Series\n",
    "        Actual values\n",
    "    y_pred : array\n",
    "        Predicted values\n",
    "    metrics : dict\n",
    "        Dictionary of metrics\n",
    "    save_path : str\n",
    "        Path to save report\n",
    "    \"\"\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"SOLAR POWER FORECAST REPORT\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Report Generated: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Forecast Period: {y_test.index[0]} to {y_test.index[-1]}\\n\")\n",
    "        f.write(f\"Number of Predictions: {len(y_test)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE METRICS\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric:20s}: {value:.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"Report saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute main pipeline\n",
    "    results = main()\n",
    "    \n",
    "    print(\"\\n✓ Pipeline execution complete!\")\n",
    "    print(\"\\nTo access results:\")\n",
    "    print(\"  - Models: results['models']['rf'] or results['models']['gb']\")\n",
    "    print(\"  - Predictions: results['predictions']\")\n",
    "    print(\"  - Metrics: results['metrics']\")\n",
    "    print(\"  - Data splits: results['data']\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE FOR OPERATIONAL FORECASTING\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Example workflow for operational use:\n",
    "\n",
    "# 1. Train model once\n",
    "results = main()\n",
    "best_model = results['best_model']\n",
    "feature_cols = results['data']['X_test'].columns.tolist()\n",
    "\n",
    "# 2. Save model for reuse\n",
    "save_model(best_model, 'solar_forecast_model.pkl')\n",
    "\n",
    "# 3. Later, load and use for predictions\n",
    "loaded_model = load_model('solar_forecast_model.pkl')\n",
    "next_day_forecast = predict_next_day(loaded_model, latest_data, feature_cols)\n",
    "print(f\"Tomorrow's forecast: {next_day_forecast:.3f} kW\")\n",
    "\n",
    "# 4. Generate reports\n",
    "create_forecast_report(y_test, y_pred, metrics)\n",
    "\"\"\"\n",
    "results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
